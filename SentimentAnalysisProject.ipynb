{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2b5c7a23-e1a4-44de-9262-6ea05d65cd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7ef18f50-581f-4d19-87ad-2742d2eb32ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.txt\" , sep=';' ,header=None , names=['text' , 'emotion']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "14e71126-1362-4d8b-b8d9-06c0d85e9163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  emotion\n",
       "0                            i didnt feel humiliated  sadness\n",
       "1  i can go from feeling so hopeless to so damned...  sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong    anger\n",
       "3  i am ever feeling nostalgic about the fireplac...     love\n",
       "4                               i am feeling grouchy    anger"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "da6cf29f-745e-47f9-aa1a-d067be306a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for make it understanding to computer we have to classify 'emotion' into numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "61ca13d9-2fdf-4c09-a887-e6681f10de27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text       0\n",
       "emotion    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c98c7dd2-7cbf-43a6-b566-f1f3206051ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_emotions = df['emotion'].unique() #6 unique emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3642fdac-a7bf-47d3-ad10-20181fd9df53",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_num = {}\n",
    "i = 0\n",
    "for emo in unique_emotions: \n",
    "    emotion_num[emo] = i \n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6eb41942-51f3-4b5e-8c24-7071305f3d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sadness': 0, 'anger': 1, 'love': 2, 'surprise': 3, 'fear': 4, 'joy': 5}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a5f19bcc-6787-4b8d-aed1-9a16ec4790c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emotion'] = df['emotion'].map(emotion_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "95a5d899-9189-4dcf-99cb-db04c82bccb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        1\n",
       "3        2\n",
       "4        1\n",
       "        ..\n",
       "15995    0\n",
       "15996    0\n",
       "15997    5\n",
       "15998    1\n",
       "15999    0\n",
       "Name: emotion, Length: 16000, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "aac3c0ef-1773-40e0-bfbd-e766525b27cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other things Like coverting texts to lowercase and so others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f77fdc6f-cefc-4c7d-ba98-bd7c3b838a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "db237abd-3274-45c5-a095-30ff6bf8c9d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  i didnt feel humiliated\n",
       "1        i can go from feeling so hopeless to so damned...\n",
       "2         im grabbing a minute to post i feel greedy wrong\n",
       "3        i am ever feeling nostalgic about the fireplac...\n",
       "4                                     i am feeling grouchy\n",
       "                               ...                        \n",
       "15995    i just had a very brief time in the beanbag an...\n",
       "15996    i am now turning and i feel pathetic that i am...\n",
       "15997                       i feel strong and good overall\n",
       "15998    i feel like this was such a rude comment and i...\n",
       "15999    i know a lot but i feel so stupid because i ca...\n",
       "Name: text, Length: 16000, dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "022f645c-619b-463e-beaa-816dd6e5d4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuations (used mostly in ML not in DL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6b86da1e-c35f-4410-b431-40944989311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uses string functions like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "422ccd27-38f3-4516-aa7a-5bfc65a37a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c49432e1-55c3-4edd-8e83-c21e94f9f116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_punc(text): \n",
    "    return text.translate(str.maketrans('' , '' , string.punctuation)) \n",
    "\n",
    "df['text'] = df['text'].apply(rem_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dff6cc69-9e8b-4c4b-a8a0-32d76a1e6253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i didnt feel humiliated'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d79d81e6-0f63-46d8-a549-f05a0cd6c0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now removing numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c0c32977-25a7-46cd-81b7-623a1689653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_num(text): \n",
    "    new = ''\n",
    "    for i in text: \n",
    "        if not i.isdigit():\n",
    "            new = new+ i \n",
    "    return new \n",
    "\n",
    "df['text'] = df['text'].apply(rem_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "14af0ca8-23e8-42a4-91a6-b869b980c6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now remving links/URLs (as the data is downloaded from kaggle so it doesn't have any links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9aa06fdc-d196-4759-bb79-f4049341fee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#but in case you still want to do that you can use same logic as upper and for words like 'http//'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32200d5-1875-484d-b2e3-27eaffcee1be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c1832fe6-bb3d-4394-8dd7-6beeb8f97577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now dealing with emojis or special characters we can use ascii values to identify them and then remove them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8ae250ea-dc49-4f12-b58a-be7a47aafb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#as very character has its ascii value in python which is understandable to computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dd24da49-8775-423a-b7ef-b177c3eca3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ascii values are for characters that already exists but the unicode are for characters like emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "02bff786-5910-42bc-95fb-9770a6892e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_emo(text): \n",
    "    new = ''\n",
    "    for i in text: \n",
    "        if i.isascii():\n",
    "            new = new+ i \n",
    "    return new \n",
    "\n",
    "df['text'] = df['text'].apply(rem_emo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "41fe51f4-7aed-4246-8b95-ebf7bfcab1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the final thing is to remove stop-words because its important in ML but not for Dl its helpfull for understanding context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4ce073d7-89c0-417d-bb6c-24f68db88160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for that we use libraries here ex. nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d1495ebd-22ea-45ad-96f1-46139e04b1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk #NaturallanguageToolkit And There is one another Library called spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7f7e7c61-894c-437f-a440-7ca2e2cbd2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3456715e-89ee-4875-8ae3-2f43e5c6deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c2a9a2a5-5eb6-480d-9dd6-f99e86866c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WordListCorpusReader in 'C:\\\\Users\\\\hp\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords'>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7b54fff0-843c-4680-96f2-82b15bfdd75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') #to tokenize words\n",
    "nltk.download('stopwords') #contains stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e13bd12e-52ad-484f-8d55-c151206982e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop__words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ffe9395d-7c9b-463c-87be-95c4a9cd52b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop__words #all the stop words we need to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6c1edf13-6f64-497d-b16c-814582872ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop__words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941c2ee3-fc2d-48fe-9424-119d833d92e8",
   "metadata": {},
   "source": [
    "#example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ff90b057-e68c-4d9f-8838-80dda971b4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[1]['text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d2f3e15f-a1b8-4c70-9888-deb0ebd64322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we need to convert str into array and by comparing values in the stop__words we can remove them in array itself and them convert it back to str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6219f39b-1d9c-4374-90a0-dc07c489a52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this procedure for reading of words uniquely in form of str to array is called tokenization #similar to split but tokenization is better for ML scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9912ab42-227a-475d-8fc4-4dd07782d9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tokenization and removing stop__words we will use function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1371154f-b9dd-4e32-b38c-2981eaae4f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove(text): \n",
    "    words = word_tokenize(text) #works onlt if punkt is downloaded and if you cant find punkt use \"text.split()\"\n",
    "    cleaned = []    \n",
    "    for i in words: \n",
    "        if i not in stop__words: \n",
    "            cleaned.append(i)\n",
    "    return ' '.join(cleaned)\n",
    "\n",
    "df['text'] = df['text'].apply(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d0775021-882d-4502-bcea-fc4de0b5db42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#errored so do, \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# download required nltk data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# define stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# define function\n",
    "def remove(text):\n",
    "    words = word_tokenize(text)\n",
    "    cleaned = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(cleaned)\n",
    "\n",
    "# example dataframe\n",
    "df2 = pd.DataFrame({'text': ['This is an example sentence for cleaning text data.']})\n",
    "\n",
    "# apply the function\n",
    "df['text'] = df['text'].apply(remove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4b343227-7452-4fba-9e3a-aee695b7703c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "667d0cdb-23e0-4e9e-b783-e435a066069f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go feeling hopeless damned hopeful around someone cares awake'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7fadc6af-0dc6-4f6f-ae38-b46cd94ffd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#So this was all about NLP text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3c26cc-55d1-40bc-a071-1a86ad47ab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "64bad658-3be2-452b-bf8a-6618022c7a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will Complete this project using TF-IDF Vectorization technique \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ce7154d3-2880-4f61-999a-7899d7593e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2b5af493-3370-40d7-bf26-5c57fb36c942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "90c66277-df70-4d32-9ac6-b3316ef0586e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>go feeling hopeless damned hopeful around some...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing minute post feel greedy wrong</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ever feeling nostalgic fireplace know still pr...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feeling grouchy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  emotion\n",
       "0                              didnt feel humiliated        0\n",
       "1  go feeling hopeless damned hopeful around some...        0\n",
       "2          im grabbing minute post feel greedy wrong        1\n",
       "3  ever feeling nostalgic fireplace know still pr...        2\n",
       "4                                    feeling grouchy        1"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "30f6ef55-bad0-4d15-b830-2e6b5748516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df['text'], df['emotion'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4115e909-af90-4695-8628-ba39f196edd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "676      refers course though cant help feeling somehow...\n",
       "12113                im starting feel im suffering fatigue\n",
       "7077     feel like probably would liked book little bit...\n",
       "13005                                  really feel awkward\n",
       "12123    im feeling little grumpy today lame weather te...\n",
       "                               ...                        \n",
       "13418    love leave reader feeling confused slightly de...\n",
       "5390                                         feel delicate\n",
       "860                          starting feel little stressed\n",
       "15795             feel stressed tired worn shape neglected\n",
       "7270         feel someone rude wrongly done something lose\n",
       "Name: text, Length: 12800, dtype: object"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0577cb77-8746-4d16-bbda-9c0c2cefbdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train and x_test to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "768ffce6-f410-4dc5-9927-4f44c0c178b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with both BoW and TF-IDf and will check which model performs better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0e23f6ea-dd85-4c76-9b89-5286b3f354c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (BoW + MultinomialNB): 0.8616\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- First model using Bag of Words (BoW) ---\n",
    "BoW = CountVectorizer()\n",
    "\n",
    "# Fit on training data, transform both train and test\n",
    "x_train_BoW = BoW.fit_transform(x_train)\n",
    "x_test_BoW = BoW.transform(x_test)  # <-- Corrected this line\n",
    "\n",
    "# --- Train a Multinomial Naive Bayes model ---\n",
    "MNB_model = MultinomialNB()\n",
    "MNB_model.fit(x_train_BoW, y_train)\n",
    "\n",
    "# --- Make predictions ---\n",
    "MNB_BoW_pred = MNB_model.predict(x_test_BoW)\n",
    "\n",
    "# --- Evaluate accuracy ---\n",
    "accuracy_BoW_MultinomialNB = accuracy_score(y_test, MNB_BoW_pred)\n",
    "print(f\"Accuracy (BoW + MultinomialNB): {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "fd28bef5-8d44-4389-9630-1cf4f21df614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<12800x13359 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 116049 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ead830-6ea2-447a-98f3-4aef05778671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e32c9233-d93f-4e5c-a358-a1e50872c71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (TF-IDF + MultinomialNB): 0.8616\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- TF-IDF Vectorization ---\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "# Fit on training data, transform both train and test\n",
    "x_train_tf = tf_idf.fit_transform(x_train)\n",
    "x_test_tf = tf_idf.transform(x_test)\n",
    "\n",
    "# --- Train Multinomial Naive Bayes model ---\n",
    "tfidf_model = MultinomialNB()\n",
    "tfidf_model.fit(x_train_tf, y_train)\n",
    "\n",
    "# --- Make predictions ---\n",
    "MNB_tfidf_pred = tfidf_model.predict(x_test_tf)\n",
    "\n",
    "# --- Evaluate accuracy ---\n",
    "accuracy_TF_IDF_MultinomialNB = accuracy_score(y_test, MNB_tfidf_pred)\n",
    "print(f\"Accuracy (TF-IDF + MultinomialNB): {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0eba3583-f9f0-423a-a4cf-962d38094a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we can see that models accuracy was good when we used BoW Vectorixzation techniques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4f8ed3-3f42-4dba-9066-d6cda03d9bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3022f7e6-3117-4120-a121-0489861b084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#but one more thing we can do is that we can use Logistic Regression technique with Tf-IDF cause we know Logistic Regression uses log-loss function Which consist of values In probability range 0 to 1 which is same as vector points in the Tf-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "64b014f4-9655-4d3f-9c3f-a6e9874a73e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (TF-IDF + Logistic Regression): 0.8616\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- TF-IDF Vectorization ---\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "# Fit on training data, transform both train and test\n",
    "x_train_tf = tf_idf.fit_transform(x_train)\n",
    "x_test_tf = tf_idf.transform(x_test)\n",
    "\n",
    "# --- Train Logistic Regression model ---\n",
    "tfidf_model2 = LogisticRegression(max_iter=1000)\n",
    "tfidf_model2.fit(x_train_tf, y_train)\n",
    "\n",
    "# --- Make predictions ---\n",
    "LR_tfidf_pred = tfidf_model2.predict(x_test_tf)\n",
    "\n",
    "# --- Evaluate accuracy ---\n",
    "accuracy_TF_IDF_LogisticRegression = accuracy_score(y_test, LR_tfidf_pred)\n",
    "print(f\"Accuracy (TF-IDF + Logistic Regression): {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e15c076d-edaf-47e3-9ede-a45dbe0fc700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#so here we can see The Tf-Idf with LogisticRegression gives best accuracy till now Compared to Tf-Idf with MultinomialNB And BoW with MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ba580939-b5e1-47ae-bca5-77a9f765964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final Conclusion Scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "01fe9db7-9697-44ad-b2c9-5eed8b0213a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7678125"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_BoW_MultinomialNB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "fda80b32-8511-49be-bb86-166f5309c877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6609375"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_TF_IDF_MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "970c34c8-5c32-4b3d-b23a-46cb37a448a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8615625"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_TF_IDF_LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "99f5bbec-4cfb-41d5-93c5-3a237958f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will use Model Tuning And Hyper ParametersTuning Later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3f765e-4a8f-4108-9bdf-886f4c929e16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
